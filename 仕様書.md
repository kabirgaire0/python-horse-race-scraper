# 仕様書

## 目的
スクレイピングしたデータをCSVファイルに保存する。
スクレイピング対象のデータは以下の通り。

### 開催データ
中央競馬・地方競馬の2種類の開催データを出力する。
出力項目：
- 開催キー
- 開催日
- 競馬場コード
- 開催回
- 開催日目

### レースデータ
中央競馬・地方競馬・ばんえい競馬の3種類にカテゴライズして出力する。
ばんえい競馬は、地方競馬のレースデータをスクレイピングする際に、会場が帯広競馬場の場合に分岐するようにする。
出力項目：
- レースキー
- 競馬場コード
- 開催回
- 開催日
- 開催日目
- レース番号
- 競走種別
- 競走条件
- 競走記号
- 重量種別
- グレード
- 競走名
- 競走名短縮
- 距離
- トラック種別
- 馬場状態
- 天候
- 出走頭数
- 発走時刻
- 登録頭数

### 出馬データ
中央競馬・地方競馬・ばんえい競馬の3種類にカテゴライズして出力する。
「出馬表」ページから、出馬データをスクレイピングする。
出馬データと印データは別に格納する。
テーブルのカラムと出力データのカラムを対応させる。
出力項目（出馬データ）：
- レースキー
- 馬番
- 枠番
- 競走馬コード
- 競走馬名
- 性別
- 年齢
- 斤量
- 騎手コード
- 騎手名
- 調教師コード
- 調教師名
- 所属
- 馬体重
- 馬体重増減

出力項目（印データ）：
- レースキー
- 馬番
- 印

### 調教データ
「調教」ページから、各種調教データを出力する。
出走レース間のデータのみを出力する。調教に付随する併せ馬データも、カラムを追加して一緒に格納する。
出力項目：
- レースキー
- 競走馬コード
- 調教年月日
- 調教時刻
- 調教コース
- 調教タイム
- 併せ馬情報

### オッズデータ
「オッズ」ページから、各種オッズデータを出力する。
単復は3カラム、ワイドは2カラムで格納する。
馬複・馬単・三連単・三連複のオッズデータは、オッズを7桁×18頭分に圧縮して格納するようにする。
出力項目：
- レースキー
- 馬番
- 単勝オッズ
- 複勝オッズ下限
- 複勝オッズ上限
- ワイドオッズ下限
- ワイドオッズ上限
- 馬複オッズ（圧縮）
- 馬単オッズ（圧縮）
- 三連複オッズ（圧縮）
- 三連単オッズ（圧縮）

### ポイントデータ
「ポイント」ページから、各種ポイントデータを出力する。
出力項目：
- レースキー
- 競走馬コード
- ポイント

### パドック情報データ
「パドック情報」ページから、各種パドック情報データを出力する。
出力項目：
- レースキー
- 競走馬コード
- コメント
- 人気程
- 評価

### ギリギリ情報データ
「ギリギリ情報」ページから、各種ギリギリ情報データを出力する。
出力項目：
- レースキー
- 競走馬コード
- TM
- タイトル
- 本文

### TM直前情報データ
「TM直前情報」ページから、各種TM直前情報データを出力する。
出力項目：
- レースキー
- 競走馬コード
- TM
- タイトル
- 本文


### 厩舎の話データ
「厩舎の話」ページから、各種厩舎の話データを出力する。
出力項目：
- レースキー
- 競走馬コード
- 厩舎コメント

### レース結果データ
「レース結果」ページから、各種レース結果データを出力する。
中央競馬・地方競馬・ばんえい競馬の3種類にカテゴライズして出力する。
レース結果ページには、レース結果・その他の付随情報・レース後の各種コメントが含まれており、それぞれ別に格納する。
出力項目（レース結果）：
- レースキー
- 着順
- 馬番
- 枠番
- 競走馬コード
- 競走馬名
- 性別
- 年齢
- 斤量
- 騎手コード
- 騎手名
- タイム
- 着差
- 単勝オッズ
- 人気
- 馬体重
- 馬体重増減
- 調教師コード
- 調教師名
- 所属

出力項目（付随情報）：
- レースキー
- コーナー通過順
- ラップタイム
- 上がり3ハロンタイム

出力項目（レース後コメント）：
- レースキー
- 競走馬コード
- コメント種別
- コメント内容


## ①ファイルの概要

### scrape_racedata.py
- BeautifulSoupを使用してレースデータをスクレイピングし、CSVファイルに保存するための関数群を提供します。

### scraping_module.py
- ページのデータをスクレイピングし、CSVファイルに保存するための汎用的な関数を提供します。また、複数層のスクレイピングを行うための関数も含まれています。

### config.py
- スクレイピングに必要な設定情報を提供します。スクレイピング期間、ログイン情報、CSSセレクタ、CSVファイルのパスなどが含まれます。

### main.py
- スクレイピングのメインスクリプトです。ドライバの設定、ログイン、スクレイピングの実行、データの保存を行います。

### driver_setup.py
- Selenium WebDriverのセットアップとログイン処理を行うための関数を提供します。

## ②関数の概要

### scrape_racedata.py

#### scrape_racedata
- 指定されたレースキーに基づいて適切なスクレイピング関数を呼び出します。
  1. レースキーに基づいてURLを生成。
  2. ページにアクセスし、HTMLを取得。
  3. BeautifulSoupでHTMLをパースし、メニューコンテナを取得。
  4. メニューコンテナに基づいて各ページのスクレイピング関数を呼び出す。

#### scrape_racedata_syutuba
- 'syutuba'ページをスクレイピングしてデータをCSVに保存します。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、テーブルデータを抽出。
  3. データをCSVに保存。

#### scrape_racedata_odds
- 'odds'ページをスクレイピングしてデータをCSVに保存します。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、テーブルデータを抽出。
  3. データをCSVに保存。

#### scrape_racedata_tyoukyou
- '調教'ページをスクレイピングしてデータをCSVに保存します。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、テーブルデータを抽出。
  3. データをCSVに保存。

#### scrape_racedata_paddok
- 'パドック情報'ページをスクレイピングしてデータをCSVに保存します。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、テーブルデータを抽出。
  3. データをCSVに保存。

#### scrape_racedata_girigiri
- 'ギリギリ情報'ページをスクレイピングしてデータをCSVに保存します。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、テーブルデータを抽出。
  3. データをCSVに保存。

#### scrape_racedata_cyokuzen
- 'TM直前情報'ページをスクレイピングしてデータをCSVに保存します。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、テーブルデータを抽出。
  3. データをCSVに保存。

#### scrape_racedata_danwa
- '厩舎の話'ページをスクレイピングしてデータをCSVに保存します。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、テーブルデータを抽出。
  3. データをCSVに保存。

#### scrape_racedata_point
- 'ポイント'ページをスクレイピングしてデータをCSVに保存します。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、テーブルデータを抽出。
  3. データをCSVに保存。

#### scrape_racedata_seiseki
- 'レース結果'ページをスクレイピングしてデータをCSVに保存します。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、テーブルデータを抽出。
  3. データをCSVに保存。

#### save_racedata_to_csv
- スクレイピングされたレースデータをCSVファイルに保存します。
  1. データをCSV形式に変換。
  2. 指定されたファイルパスに保存。

### scraping_module.py

#### scrape_page
- 指定されたURLにアクセスし、ページのデータをスクレイピングします。
  1. ページにアクセスし、HTMLを取得。
  2. BeautifulSoupでHTMLをパースし、指定されたCSSセレクタに基づいてデータを抽出。

#### save_to_csv
- データをCSVファイルに保存します。
  1. データをCSV形式に変換。
  2. 指定されたファイルパスに保存。

#### kaisai_date
- ピボットされたデータフレームを解除し、日付と競馬場のペアを生成し、CSVファイルに出力します。
  1. ピボットされたデータフレームを解除。
  2. 日付と競馬場のペアを生成。
  3. データをCSVに保存。

#### generate_date_strings
- 指定された期間のすべての年月を表す文字列を生成します。
  1. 開始日と終了日を受け取り、年月のリストを生成。

#### second_layer_scrape
- 二層目のスクレイピングを行い、データを取得します。
  1. 指定された日付範囲内のページをスクレイプ。
  2. データをCSVに保存。
  3. 三層目のスクレイピングを呼び出す。

#### scrape_kaisai
- 各年月のページをスクレイプし、データをCSVファイルに保存します。
  1. 指定された年月のページにアクセス。
  2. BeautifulSoupでHTMLをパースし、データを抽出。
  3. データをCSVに保存。

### config.py
- スクレイピングに必要な設定情報を提供します。
  - スクレイピング期間
  - ログイン情報
  - 一層目、二層目のスクレイピング設定
  - CSVファイルのパス
  - syutubaのカラム設定

### main.py

#### main
- スクレイピングのメインスクリプトです。
  1. ドライバの設定とログイン。
  2. スクレイピング期間の年月を生成。
  3. 各年月のページをスクレイプし、データをCSVに保存。
  4. 二層目のスクレイピングを実行。

### driver_setup.py

#### setup_driver
- Selenium WebDriverをセットアップし、ヘッドレスモードでFirefoxブラウザを起動します。
  1. Firefoxのオプションを設定し、ヘッドレスモードを有効化。
  2. WebDriverを起動。

#### random_delay
- 最小および最大遅延時間の間でランダムな遅延を挿入します。
  1. 指定された範囲内でランダムな時間を生成。
  2. 生成された時間だけスリープ。

#### login_keibabook
- 指定されたURLにアクセスし、ユーザー名とパスワードを使用してログインします。
  1. ログインページにアクセス。
  2. ユーザー名とパスワードを入力。
  3. ログインボタンをクリック。
  4. ランダムな遅延を挿入。